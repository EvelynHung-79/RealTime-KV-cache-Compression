import sys
import os
import json
import torch
import pytest # å¼•å…¥ pytest ä»¥ä¾¿å¯èƒ½ä½¿ç”¨å…¶åŠŸèƒ½ï¼Œé›–ç„¶æ­¤è…³æœ¬ä¸»è¦æ˜¯åŸ·è¡Œæ€§çš„

# ===== è·¯å¾‘è¨­ç½® =====
# ç¢ºä¿å¯ä»¥æ‰¾åˆ° src å’Œ configs ç›®éŒ„ä¸‹çš„æ¨¡çµ„
sys.path.insert(0, os.path.abspath(os.path.join(os.path.dirname(__file__), '..')))

from src.models.modified_llama import create_compressed_llama_model
from src.evaluation.longbench_eval import LongBenchEvaluator
from configs.base_config import CompressionConfig
from transformers import AutoTokenizer

# --- æ¸¬è©¦é…ç½® ---
# ä½¿ç”¨è¼ƒå°çš„æ¨¡å‹æˆ–ç¢ºä¿æœ‰è¶³å¤ è³‡æºé‹è¡Œ Llama-2-7b
# æ³¨æ„ï¼šçœŸå¯¦æ¨¡å‹è·¯å¾‘éœ€è¦æŒ‡å‘æ‚¨ä¸‹è¼‰çš„æ¨¡å‹
MODEL_PATH = "meta-llama/Llama-2-7b-hf" # æˆ–è€…æ‚¨çš„æœ¬åœ°è·¯å¾‘ "models/llama2-7b"
# MODEL_PATH = "TinyLlama/TinyLlama-1.1B-Chat-v1.0" # å‚™é¸ï¼šä½¿ç”¨æ›´å°çš„æ¨¡å‹é€²è¡Œå¿«é€Ÿæ¸¬è©¦

# æª¢æŸ¥æ¨¡å‹è·¯å¾‘æ˜¯å¦å­˜åœ¨ (å¦‚æœæ˜¯æœ¬åœ°è·¯å¾‘)
# if not os.path.isdir(MODEL_PATH):
#     pytest.skip(f"Model path '{MODEL_PATH}' not found, skipping functionality test.", allow_module_level=True)

# ===== 1ï¸âƒ£ CUDA ç’°å¢ƒæª¢æŸ¥ =====
if torch.cuda.is_available():
    device = "cuda"
    print("âœ… CUDA is available. Running on GPU.")
else:
    device = "cpu"
    print("âš ï¸ CUDA not available. Running on CPU. This might be slow.")

# ===== 2ï¸âƒ£ æ¨¡å‹è¨­å®š (ä½¿ç”¨æ–°çš„ CompressionConfig) =====
try:
    # å‰µå»ºæ–°çš„ CompressionConfig å¯¦ä¾‹
    config = CompressionConfig(
        model_name=MODEL_PATH,
        chunk_size=256,
        ema_decay=0.99,
        # outlier_threshold_abs=6.0, # å¯ä»¥å…ˆä¸è¨­çµ•å°é–¾å€¼
        outlier_threshold_relative=5.0,
        attention_sink_size=8,
        key_bits_normal=4,
        key_bits_sink_outlier=8,
        value_bits_normal=4,
        value_bits_sink_outlier=8,
        value_quant_groups=-1 # Per-channel value quant
    )
except Exception as e:
     pytest.fail(f"Failed to initialize CompressionConfig: {e}")


# ===== 3ï¸âƒ£ åˆå§‹åŒ–æ¨¡å‹èˆ‡ tokenizer =====
print(f"ğŸš€ Loading compressed model from: {MODEL_PATH}")
try:
    # æ³¨æ„ï¼šcreate_compressed_llama_model ç¾åœ¨éœ€è¦ compression_config ä½œç‚ºåƒæ•¸
    model = create_compressed_llama_model(MODEL_PATH, compression_config=config, device=device)
    tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH)
    if tokenizer.pad_token is None:
        tokenizer.pad_token = tokenizer.eos_token
    print("âœ… Model and tokenizer loaded successfully.")
except Exception as e:
    pytest.fail(f"Failed to load model or tokenizer: {e}")


# ===== 4ï¸âƒ£ ç°¡å–®æ¸¬è©¦æ¨¡å‹æ˜¯å¦å¯ç”Ÿæˆ =====
def test_simple_generation():
    input_text = "The capital of France is Paris. Paris is known for its beautiful architecture, museums, and culture."
    inputs = tokenizer(input_text, return_tensors="pt").to(device)

    print(f"\nğŸ§ª Testing single text generation...\nInput: {input_text}")
    generated_text = "Generation failed"
    try:
        with torch.no_grad():
            # é‡ç½®çµ±è¨ˆæ•¸æ“š
            if hasattr(model, 'reset_compression_state'):
                 model.reset_compression_state()

            outputs = model.generate(
                **inputs,
                max_new_tokens=20,
                do_sample=False, # ä½¿ç”¨ç¢ºå®šæ€§ç”Ÿæˆä»¥æ–¹ä¾¿æ¸¬è©¦
                pad_token_id=tokenizer.eos_token_id
            )
            generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)
            print(f"Output: {generated_text}")
            assert len(generated_text) > len(input_text) # åŸºæœ¬æª¢æŸ¥ï¼šæ˜¯å¦ç”Ÿæˆäº†æ–°å…§å®¹
    except Exception as e:
        pytest.fail(f"Simple generation failed: {e}")
    finally:
        print(f"Raw Output: {generated_text}") # æ‰“å°åŸå§‹è¼¸å‡ºä»¥ä¾›èª¿è©¦

    # ç²å–æ–°çš„çµ±è¨ˆæ•¸æ“š (å…§å®¹å·²æ”¹è®Š)
    compression_stats = {}
    if hasattr(model, 'get_compression_stats'):
        compression_stats = model.get_compression_stats()
        print(f"\nğŸ“Š Compression Stats: {compression_stats}")
        # å¯ä»¥åŠ å…¥å° stats å…§å®¹çš„åŸºæœ¬æ–·è¨€ï¼Œä¾‹å¦‚ key æ˜¯å¦å­˜åœ¨
        assert isinstance(compression_stats, dict)
    else:
        print("\nâš ï¸ Model does not have get_compression_stats method.")

    print("-" * 80)

# ===== 5ï¸âƒ£ LongBench è©•æ¸¬ (ç¸®æ¸›ç‰ˆæ¸¬è©¦) =====
# æ³¨æ„ï¼šé‹è¡Œæ­¤æ¸¬è©¦éœ€è¦ä¸‹è¼‰ LongBench æ•¸æ“šé›†ä¸¦å¯èƒ½æ¶ˆè€—è¼ƒé•·æ™‚é–“å’Œè³‡æº
@pytest.mark.skip(reason="LongBench evaluation can be time-consuming and requires dataset.")
def test_longbench_evaluation_short():
    print("\nğŸ Starting LongBench Evaluation (short test)...")

    # å®šç¾©è¼¸å‡ºç›®éŒ„
    TEST_OUTPUT_DIR = "experiments/results/test_functionality_output_pytest"
    os.makedirs(TEST_OUTPUT_DIR, exist_ok=True)

    try:
        # åˆå§‹åŒ– evaluator
        evaluator = LongBenchEvaluator(model, tokenizer, config, output_dir=TEST_OUTPUT_DIR)

        # --- å–®ä¸€ä»»å‹™å¿«é€Ÿæ¸¬è©¦ ---
        print(f"\nğŸ¯ Evaluating single task (narrativeqa), results saved to {TEST_OUTPUT_DIR}...")
        single_task_result = evaluator.evaluate_task(
            'narrativeqa',
            max_samples=2, # ä½¿ç”¨éå¸¸å°‘çš„æ¨£æœ¬ä»¥åŠ é€Ÿ
            max_new_tokens=10 # ä½¿ç”¨éå¸¸å°‘çš„æ–° token
        )

        print("\n--- Single Task Result (narrativeqa) ---")
        print(json.dumps(single_task_result, indent=2, ensure_ascii=False))
        # åŸºæœ¬æ–·è¨€ï¼šçµæœæ‡‰è©²åŒ…å«é æœŸçš„ key
        assert 'task_name' in single_task_result
        assert 'num_samples' in single_task_result
        assert 'quality_metrics' in single_task_result
        assert 'compression_metrics' in single_task_result # è©•ä¼°å™¨èšåˆå¾Œçš„æŒ‡æ¨™
        assert single_task_result['num_samples'] == 2

    except Exception as e:
        pytest.fail(f"LongBench evaluation failed: {e}")
    finally:
        print(f"\nâœ… Evaluation test completed. Detailed results (if run) saved in: {TEST_OUTPUT_DIR}\n")

# å¯ä»¥æ·»åŠ æ›´å¤šæ¸¬è©¦ï¼Œä¾‹å¦‚æ¸¬è©¦ reset_compression_state çš„æ•ˆæœç­‰

# å¦‚æœå¸Œæœ›ç›´æ¥é‹è¡Œæ­¤æ–‡ä»¶é€²è¡Œæ¸¬è©¦ (é pytest)
if __name__ == "__main__":
    print("Running basic generation test...")
    test_simple_generation()
    # print("\nRunning LongBench short test (if uncommented)...")
    # test_longbench_evaluation_short() # å–æ¶ˆè¨»é‡‹ä»¥é‹è¡Œè©•ä¼°æ¸¬è©¦