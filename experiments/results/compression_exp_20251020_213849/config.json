{
  "compression_config": {
    "model_name": "models/llama2-7b",
    "max_position_embeddings": 4096,
    "num_hidden_layers": 32,
    "hidden_size": 4096,
    "num_attention_heads": 32,
    "alpha": 0.6,
    "beta": 0.2,
    "gamma": 0.2,
    "theta_h": 0.6,
    "theta_m": 0.2,
    "layer_weights": [
      1.0,
      0.9838709677419355,
      0.967741935483871,
      0.9516129032258065,
      0.935483870967742,
      0.9193548387096774,
      0.9032258064516129,
      0.8870967741935484,
      0.8709677419354839,
      0.8548387096774194,
      0.8387096774193549,
      0.8225806451612903,
      0.8064516129032258,
      0.7903225806451613,
      0.7741935483870968,
      0.7580645161290323,
      0.7419354838709677,
      0.7258064516129032,
      0.7096774193548387,
      0.6935483870967742,
      0.6774193548387097,
      0.6612903225806452,
      0.6451612903225806,
      0.6290322580645161,
      0.6129032258064516,
      0.5967741935483871,
      0.5806451612903225,
      0.564516129032258,
      0.5483870967741935,
      0.532258064516129,
      0.5161290322580645,
      0.5
    ],
    "early_layer_ratio": 0.8,
    "middle_layer_ratio": 0.6,
    "later_layer_ratio": 0.4,
    "high_precision_bits": 8,
    "medium_precision_bits": 4,
    "low_precision_bits": 2,
    "memory_budget_ratio": 0.5,
    "quality_loss_tolerance": 0.05,
    "context_lengths": [
      4096
    ],
    "batch_sizes": [
      1
    ]
  },
  "args": {
    "model_name": "models/llama2-7b",
    "device": "cuda",
    "max_length": 4096,
    "alpha": 0.6,
    "beta": 0.2,
    "gamma": 0.2,
    "theta_h": 0.6,
    "theta_m": 0.2,
    "early_ratio": 0.8,
    "middle_ratio": 0.6,
    "later_ratio": 0.4,
    "tasks": [
      "narrativeqa",
      "qasper",
      "multifieldqa_en"
    ],
    "max_samples": 10,
    "max_new_tokens": 100,
    "output_dir": "./experiments/results",
    "experiment_name": "compression_exp_20251020_213849",
    "save_model": false,
    "baseline": true
  }
}