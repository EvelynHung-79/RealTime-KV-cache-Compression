# å¿«é€Ÿå…¥é–€æŒ‡å—

## å®‰è£æ­¥é©Ÿ

### 1. è§£å£“å°ˆæ¡ˆ
```bash
unzip real-time-prefill-kv-cache-compression.zip
cd real-time-prefill-kv-cache-compression
```

### 2. è¨­ç½®ç’°å¢ƒ
```bash
# æ–¹æ³• A: ä½¿ç”¨æä¾›çš„è…³æœ¬
bash scripts/setup_environment.sh

# æ–¹æ³• B: æ‰‹å‹•è¨­ç½®
python3 -m venv venv
source venv/bin/activate  # Linux/Mac
# æˆ– venv\Scripts\activate  # Windows

pip install -r requirements.txt
# pip install -e .
```

### 3. ä¸‹è¼‰æ¨¡å‹ï¼ˆå¯é¸ï¼‰
```bash
python scripts/download_model.py
```

## åŸºæœ¬ä½¿ç”¨

### é‹è¡Œç°¡å–®å¯¦é©—
```bash
python experiments/run_compression_experiment.py \
    --model_name meta-llama/Llama-2-7b-hf \
    --chunk_size 512 \
    --key_bits_normal 4 \
    --value_bits_normal 4 \
    --attention_sink_size 4 \
    --outlier_detection_enabled \
    --max_samples 10
```

### é‹è¡Œæ¶ˆèå¯¦é©—
```bash
python experiments/ablation_study.py \
    --model_name meta-llama/Llama-2-7b-hf \
    --output_dir ablation_results/
```

### é‹è¡Œè¶…åƒæ•¸èª¿å„ª
```bash
python experiments/hyperparameter_tuning.py \
    --model_name meta-llama/Llama-2-7b-hf \
    --n_trials 20
```

## åœ¨ Python ä¸­ä½¿ç”¨

```python
from src.models.modified_llama import CompressedLlamaForCausalLM
from configs.base_config import CompressionConfig
from transformers import AutoTokenizer
import torch

# é…ç½®å£“ç¸®åƒæ•¸
config = CompressionConfig(
    chunk_size=512,
    ema_decay=0.9,
    attention_sink_size=4,
    key_bits_normal=4,
    value_bits_normal=4,
    key_bits_sink_outlier=8,
    value_bits_sink_outlier=8,
    outlier_threshold_relative=3.0,
    outlier_detection_enabled=True,
)

# è¼‰å…¥æ¨¡å‹
model = CompressedLlamaForCausalLM.from_pretrained(
    "meta-llama/Llama-2-7b-hf",
    compression_config=config,
    torch_dtype=torch.float16,
    device_map="auto",
)

tokenizer = AutoTokenizer.from_pretrained("meta-llama/Llama-2-7b-hf")

# ç”Ÿæˆæ–‡æœ¬
prompt = "Once upon a time"
inputs = tokenizer(prompt, return_tensors="pt").to("cuda")
outputs = model.generate(**inputs, max_new_tokens=50)
print(tokenizer.decode(outputs[0]))

# æŸ¥çœ‹å£“ç¸®çµ±è¨ˆ
stats = model.get_compression_stats()
print(f"å¹³å‡ Key ä½å…ƒæ•¸: {stats['avg_key_bits']:.2f}")
print(f"å¹³å‡ Value ä½å…ƒæ•¸: {stats['avg_value_bits']:.2f}")
print(f"å£“ç¸®æ¯”: {stats['compression_ratio_key']:.2f}x")
```

## é‹è¡Œæ¸¬è©¦

```bash
# é‹è¡Œæ‰€æœ‰æ¸¬è©¦
pytest tests/ -v

# é‹è¡Œç‰¹å®šæ¸¬è©¦
pytest tests/test_quantization.py -v
pytest tests/test_compression.py -v
```

## å°ˆæ¡ˆçµæ§‹

```
real-time-prefill-kv-cache-compression/
â”œâ”€â”€ README.md                  # å°ˆæ¡ˆç¸½è¦½
â”œâ”€â”€ QUICKSTART.md              # æœ¬æ–‡æª”
â”œâ”€â”€ requirements.txt           # ä¾è³´å¥—ä»¶
â”œâ”€â”€ setup.py                   # å®‰è£è¨­å®š
â”œâ”€â”€ configs/                   # é…ç½®æ¨¡çµ„
â”‚   â””â”€â”€ base_config.py         # æ ¸å¿ƒé…ç½®
â”œâ”€â”€ src/                       # æºä»£ç¢¼
â”‚   â”œâ”€â”€ models/                # æ¨¡å‹å¯¦ç¾
â”‚   â”‚   â”œâ”€â”€ modified_llama.py  # å£“ç¸®ç‰ˆ Llama
â”‚   â”‚   â””â”€â”€ compression_layers.py
â”‚   â”œâ”€â”€ compression/           # é‡åŒ–æ ¸å¿ƒ
â”‚   â”‚   â”œâ”€â”€ streaming_quantization.py
â”‚   â”‚   â””â”€â”€ unified_compressor.py
â”‚   â”œâ”€â”€ utils/                 # å·¥å…·å‡½æ•¸
â”‚   â””â”€â”€ evaluation/            # è©•ä¼°æ¨¡çµ„
â”œâ”€â”€ experiments/               # å¯¦é©—è…³æœ¬
â”‚   â”œâ”€â”€ run_compression_experiment.py
â”‚   â”œâ”€â”€ ablation_study.py
â”‚   â””â”€â”€ hyperparameter_tuning.py
â”œâ”€â”€ tests/                     # å–®å…ƒæ¸¬è©¦
â””â”€â”€ scripts/                   # è¼”åŠ©è…³æœ¬
```

## é…ç½®åƒæ•¸èªªæ˜

### æ ¸å¿ƒåƒæ•¸
- `chunk_size`: ä¸²æµè™•ç†çš„æ•¸æ“šå¡Šå¤§å°ï¼ˆæ¨è–¦ï¼š512-2048ï¼‰
- `ema_decay`: æŒ‡æ•¸ç§»å‹•å¹³å‡çš„è¡°æ¸›ä¿‚æ•¸ï¼ˆæ¨è–¦ï¼š0.9-0.99ï¼‰
- `attention_sink_size`: Attention Sink tokens æ•¸é‡ï¼ˆæ¨è–¦ï¼š4-8ï¼‰

### é‡åŒ–åƒæ•¸
- `key_bits_normal`: å¸¸è¦ Key çš„é‡åŒ–ä½å…ƒæ•¸ï¼ˆæ¨è–¦ï¼š3-4ï¼‰
- `value_bits_normal`: å¸¸è¦ Value çš„é‡åŒ–ä½å…ƒæ•¸ï¼ˆæ¨è–¦ï¼š3-4ï¼‰
- `key_bits_sink_outlier`: Sink/Outlier Key çš„ä½å…ƒæ•¸ï¼ˆæ¨è–¦ï¼š8ï¼‰
- `value_bits_sink_outlier`: Sink/Outlier Value çš„ä½å…ƒæ•¸ï¼ˆæ¨è–¦ï¼š8ï¼‰

### Outlier æª¢æ¸¬
- `outlier_threshold_relative`: ç›¸å°é–¾å€¼å€æ•¸ï¼ˆæ¨è–¦ï¼š2.5-3.5ï¼‰
- `outlier_detection_enabled`: æ˜¯å¦å•Ÿç”¨ Outlier æª¢æ¸¬

## å¸¸è¦‹å•é¡Œ

### Q: è¨˜æ†¶é«”ä¸è¶³æ€éº¼è¾¦ï¼Ÿ
A: å˜—è©¦ï¼š
1. æ¸›å° `chunk_size`
2. ä½¿ç”¨è¼ƒä½çš„é‡åŒ–ä½å…ƒæ•¸
3. ä½¿ç”¨ `torch.float16` æˆ– `bfloat16`
4. å•Ÿç”¨ `device_map="auto"` é€²è¡Œæ¨¡å‹åˆ†ç‰‡

### Q: å¦‚ä½•è©•ä¼°ä¸åŒé…ç½®ï¼Ÿ
A: ä½¿ç”¨æä¾›çš„æ¶ˆèå¯¦é©—è…³æœ¬ï¼š
```bash
python experiments/ablation_study.py
```

### Q: å¦‚ä½•è‡ªå®šç¾©é‡åŒ–ç­–ç•¥ï¼Ÿ
A: ä¿®æ”¹ `src/compression/streaming_quantization.py` ä¸­çš„é‡åŒ–å‡½æ•¸ï¼Œ
æˆ–å‰µå»ºæ–°çš„ `CompressionConfig`ã€‚

## ä¸‹ä¸€æ­¥

1. é‹è¡ŒåŸºç¤å¯¦é©—é©—è­‰è¨­ç½®
2. å˜—è©¦ä¸åŒçš„é…ç½®åƒæ•¸
3. åœ¨ä½ çš„æ•¸æ“šé›†ä¸Šè©•ä¼°
4. æ ¹æ“šçµæœèª¿æ•´è¶…åƒæ•¸
5. å¯¦ç¾è‡ªå®šç¾©åŠŸèƒ½ï¼ˆå¦‚èåˆ CUDA kernelï¼‰

## æŠ€è¡“æ”¯æŒ

å¦‚æœ‰å•é¡Œï¼Œè«‹ï¼š
1. æŸ¥çœ‹ README.md ç²å–è©³ç´°æ–‡æª”
2. æŸ¥çœ‹æ¸¬è©¦æ–‡ä»¶äº†è§£ä½¿ç”¨ç¯„ä¾‹
3. æäº¤ Issue æˆ–è¯ç¹«ä½œè€…

## å¼•ç”¨

å¦‚æœæœ¬å°ˆæ¡ˆå°ä½ çš„ç ”ç©¶æœ‰å¹«åŠ©ï¼Œè«‹å¼•ç”¨ï¼š

```bibtex
@misc{streaming-kvquant-2025,
  title={Streaming KVQuant with Sink and Outlier Awareness for Real-time Prefill KV Cache Compression},
  author={Your Name},
  year={2025},
  url={https://github.com/yourusername/real-time-prefill-kv-cache-compression}
}
```

---

**ç¥ç ”ç©¶é †åˆ©ï¼** ğŸš€
